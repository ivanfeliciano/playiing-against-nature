{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import networkx as nx\n",
    "from pgmpy.models import BayesianModel\n",
    "from pgmpy.factors.discrete import TabularCPD\n",
    "from pgmpy.inference import VariableElimination\n",
    "from pgmpy.sampling import BayesianModelSampling\n",
    "from scipy.stats import beta, dirichlet\n",
    "from operator import add\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## True Causal Model From Environment:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = BayesianModel([('Enfermedad', 'Final'), ('Tratamiento', 'Final'), ('Tratamiento', 'Reaccion'),\n",
    "                       ('Reaccion', 'Final')])\n",
    "\n",
    "cpd_e = TabularCPD(variable='Enfermedad', variable_card=2, values=[[0.7, 0.3]])\n",
    "cpd_tr = TabularCPD(variable='Tratamiento', variable_card=2, values=[[0.5, 0.5]])\n",
    "\n",
    "cpd_r_tr = TabularCPD(variable='Reaccion', variable_card=2, \n",
    "                   values=[[.7,.4], [.3,.6]], #vivir|farmaco vivir|cirugia #morir|farmaco morir|cirugia\n",
    "                   evidence=['Tratamiento'],\n",
    "                   evidence_card=[2])\n",
    "\n",
    "cpd_f_e_tr_r = TabularCPD(variable='Final', variable_card=2, \n",
    "                   values=[[.6,0,.8,0,.4,0,.9,0],\n",
    "                           [.4,1,.2,1,.6,1,.1,1]], #vivir|vivir,farmaco,a #morir|\n",
    "                  evidence=['Enfermedad', 'Tratamiento','Reaccion'],\n",
    "                  evidence_card=[2, 2,2])\n",
    "\n",
    "model.add_cpds(cpd_e, cpd_tr, cpd_r_tr, cpd_f_e_tr_r)\n",
    "\n",
    "infer = VariableElimination(model)\n",
    "sampling = BayesianModelSampling(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class true_causal_model:\n",
    "    def __init__(self,N,infer,sampling,model):\n",
    "        self.num_variables=N\n",
    "        self.infer=infer\n",
    "        self.sampling=sampling\n",
    "        self.true_model=model\n",
    "\n",
    "    def action_simulator(self,chosen_action):\n",
    "        response=dict()\n",
    "        elements = [0,1]\n",
    "        probabilities = [0.7,0.3]\n",
    "        res=np.random.choice(elements, 1, p=probabilities)\n",
    "        response['Enfermedad']= res[0]\n",
    "        response['Tratamiento']=chosen_action\n",
    "        response['Reaccion']=self.infer.map_query(['Reaccion'],evidence={'Tratamiento': response['Tratamiento'],'Enfermedad': response['Enfermedad']})['Reaccion']\n",
    "        response['Final']=self.infer.map_query(['Final'],evidence={'Tratamiento': response['Tratamiento'],'Enfermedad': response['Enfermedad'],'Reaccion':response['Reaccion']})['Final']\n",
    "        return(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class fully_informed_agent:\n",
    "    def __init__(self,model_simulator):\n",
    "        self.beliefs=dict()\n",
    "        self.beliefs['Enfermedad']=[0.7, #0\n",
    "                                    0.3] #1\n",
    "        \n",
    "        self.beliefs['Tratamiento']=[0.5, #0\n",
    "                                     0.5] #1\n",
    "        \n",
    "        self.beliefs['Reaccion|Tratamiento']=[[.7,.4], #0 \n",
    "                                              [.3,.6]] #1\n",
    "        \n",
    "        self.beliefs['Final|Enfermedad, Tratamiento, Reaccion']=[[.6,0,.8,0,.4,0,.9,0], #0\n",
    "                                                                 [.4,1,.2,1,.6,1,.1,1]] #1\n",
    "        self.recompensa=[0]\n",
    "        self.simulator=model_simulator.action_simulator\n",
    "        \n",
    "    def do_calculus(self,final,treatment): \n",
    "        #La distribuci贸n P(Final|do(Tratamiento))=P(Final|Enfermedad, Tratamiento, Reaccion)P(Reaccion|Tratamiento)P(Enfermedad)\n",
    "        #Treatment = 1 贸 0\n",
    "        #Quiero que me de la probabilidad de vivir=0 贸 =1 dado treatment=0 贸 =1\n",
    "        if (treatment==0):\n",
    "            res=self.beliefs['Final|Enfermedad, Tratamiento, Reaccion'][final][0]*self.beliefs['Reaccion|Tratamiento'][0][0]*self.beliefs['Enfermedad'][0] \\\n",
    "            +self.beliefs['Final|Enfermedad, Tratamiento, Reaccion'][final][1]*self.beliefs['Reaccion|Tratamiento'][1][0]*self.beliefs['Enfermedad'][0] \\\n",
    "            +self.beliefs['Final|Enfermedad, Tratamiento, Reaccion'][final][4]*self.beliefs['Reaccion|Tratamiento'][0][0]*self.beliefs['Enfermedad'][1] \\\n",
    "            +self.beliefs['Final|Enfermedad, Tratamiento, Reaccion'][final][5]*self.beliefs['Reaccion|Tratamiento'][1][0]*self.beliefs['Enfermedad'][1]\n",
    "        else:\n",
    "            res=self.beliefs['Final|Enfermedad, Tratamiento, Reaccion'][final][2]*self.beliefs['Reaccion|Tratamiento'][0][1]*self.beliefs['Enfermedad'][0] \\\n",
    "                +self.beliefs['Final|Enfermedad, Tratamiento, Reaccion'][final][3]*self.beliefs['Reaccion|Tratamiento'][1][1]*self.beliefs['Enfermedad'][0] \\\n",
    "                +self.beliefs['Final|Enfermedad, Tratamiento, Reaccion'][final][6]*self.beliefs['Reaccion|Tratamiento'][0][1]*self.beliefs['Enfermedad'][1] \\\n",
    "                +self.beliefs['Final|Enfermedad, Tratamiento, Reaccion'][final][7]*self.beliefs['Reaccion|Tratamiento'][1][1]*self.beliefs['Enfermedad'][1]\n",
    "        return(res)\n",
    "    \n",
    "    def make_decision(self,final=1):\n",
    "        f_0= self.do_calculus(final,0)\n",
    "        f_1=self.do_calculus(final,1)\n",
    "        if (f_0 > f_1):\n",
    "            return(0)\n",
    "        else:\n",
    "            return(1)\n",
    "        \n",
    "    def observing_external(self):\n",
    "        chosen_action=self.make_decision()\n",
    "        datos=self.simulator(chosen_action)\n",
    "        self.recompensa.append(datos['Final'])\n",
    "        return(datos)\n",
    "\n",
    "    def get_reward(self):\n",
    "        return(self.recompensa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class causal_agent:\n",
    "    def __init__(self, true_model,model_simulator, max_variables, max_valores):\n",
    "        self.max_variables=max_variables\n",
    "        self.local_model=true_model ## Este es el directamente creado con PGMPY, no la clase true_causal_model\n",
    "        self.infer=VariableElimination(self.local_model)\n",
    "        self.beliefs=dict()\n",
    "        self.K=max_valores\n",
    "        self.simulator=model_simulator.action_simulator\n",
    "        \n",
    "        self.alpha_enfermedad=np.random.rand(self.K).tolist()\n",
    "        self.alpha_tratamiento=np.random.rand(self.K).tolist()\n",
    "        self.alpha_reaccion_farmaco=np.random.rand(self.K).tolist()\n",
    "        self.alpha_reaccion_cirugia=np.random.rand(self.K).tolist()\n",
    "        self.alpha_A_f_v=np.random.rand(self.K).tolist()\n",
    "        self.alpha_A_f_m=np.random.rand(self.K).tolist()\n",
    "        self.alpha_A_c_v=np.random.rand(self.K).tolist()\n",
    "        self.alpha_A_c_m=np.random.rand(self.K).tolist()\n",
    "        self.alpha_B_f_v=np.random.rand(self.K).tolist()\n",
    "        self.alpha_B_f_m=np.random.rand(self.K).tolist()\n",
    "        self.alpha_B_c_v=np.random.rand(self.K).tolist()\n",
    "        self.alpha_B_c_m=np.random.rand(self.K).tolist()\n",
    "                \n",
    "        self.conteo_enfermedad_A=0\n",
    "        self.conteo_enfermedad_B=0\n",
    "        self.conteo_tratamiento_f=0\n",
    "        self.conteo_tratamiento_c=0\n",
    "        self.conteo_reaccion_v_f=0\n",
    "        self.conteo_reaccion_m_f=0\n",
    "        self.conteo_reaccion_v_c=0\n",
    "        self.conteo_reaccion_m_c=0\n",
    "        \n",
    "        self.conteo_v_A_f_v=0\n",
    "        self.conteo_v_A_f_m=0\n",
    "        self.conteo_v_A_c_v=0\n",
    "        self.conteo_v_A_c_m=0\n",
    "        self.conteo_v_B_f_v=0\n",
    "        self.conteo_v_B_f_m=0\n",
    "        self.conteo_v_B_c_v=0\n",
    "        self.conteo_v_B_c_m=0\n",
    "        self.conteo_m_A_f_v=0\n",
    "        self.conteo_m_A_f_m=0\n",
    "        self.conteo_m_A_c_v=0\n",
    "        self.conteo_m_A_c_m=0\n",
    "        self.conteo_m_B_f_v=0\n",
    "        self.conteo_m_B_f_m=0\n",
    "        self.conteo_m_B_c_v=0\n",
    "        self.conteo_m_B_c_m=0\n",
    "        \n",
    "        self.recompensa=[0]\n",
    "        self.rounds=0\n",
    "    \n",
    "    def belief_formation(self):\n",
    "        alpha_enfermedad=self.alpha_enfermedad\n",
    "        alpha_tratamiento=self.alpha_tratamiento\n",
    "        \n",
    "        alpha_reaccion_farmaco=self.alpha_reaccion_farmaco\n",
    "        alpha_reaccion_cirugia=self.alpha_reaccion_cirugia\n",
    "        \n",
    "        alpha_A_f_v=self.alpha_A_f_v\n",
    "        alpha_A_f_m=self.alpha_A_f_m\n",
    "        alpha_A_c_v=self.alpha_A_c_v\n",
    "        alpha_A_c_m=self.alpha_A_c_m\n",
    "        alpha_B_f_v=self.alpha_B_f_v\n",
    "        alpha_B_f_m=self.alpha_B_f_m\n",
    "        alpha_B_c_v=self.alpha_B_c_v\n",
    "        alpha_B_c_m=self.alpha_B_c_m\n",
    "        \n",
    "        \n",
    "        lista=dirichlet.rvs(alpha_enfermedad,size=1).tolist()\n",
    "        self.beliefs['Enfermedad']=lista[0]\n",
    "        \n",
    "        lista=dirichlet.rvs(alpha_tratamiento,size=1).tolist()\n",
    "        self.beliefs['Tratamiento']=lista[0]\n",
    "        \n",
    "        lista_1=dirichlet.rvs(alpha_reaccion_farmaco,size=1).tolist()\n",
    "        lista_2=dirichlet.rvs(alpha_reaccion_cirugia,size=1).tolist()\n",
    "        lista_3=np.array([lista_1[0],lista_2[0]]).transpose()\n",
    "        lista_4=lista_3.tolist()\n",
    "        self.beliefs['Reaccion|Tratamiento']=lista_4\n",
    "        \n",
    "        lista_1=dirichlet.rvs(alpha_A_f_v,size=1).tolist()\n",
    "        lista_2=dirichlet.rvs(alpha_A_f_m,size=1).tolist()\n",
    "        lista_3=dirichlet.rvs(alpha_A_c_v,size=1).tolist()\n",
    "        lista_4=dirichlet.rvs(alpha_A_c_m,size=1).tolist()\n",
    "        lista_5=dirichlet.rvs(alpha_B_f_v,size=1).tolist()\n",
    "        lista_6=dirichlet.rvs(alpha_B_f_m,size=1).tolist()\n",
    "        lista_7=dirichlet.rvs(alpha_B_c_v,size=1).tolist()\n",
    "        lista_8=dirichlet.rvs(alpha_B_c_m,size=1).tolist()\n",
    "        lista_9=np.array([lista_1[0],lista_2[0],lista_3[0],lista_4[0],lista_5[0],lista_6[0],lista_7[0],lista_8[0]]).transpose()\n",
    "        lista_10=lista_9.tolist()\n",
    "        self.beliefs['Final|Enfermedad, Tratamiento, Reaccion']=lista_10\n",
    "        print(\"Creencias formadas\")\n",
    "        return(self.beliefs)\n",
    "    \n",
    "    def print_beliefs(self):\n",
    "        return(self.beliefs)\n",
    "    \n",
    "    def fix_causal_model(self):\n",
    "        model1=self.local_model\n",
    "        model=model1.copy()\n",
    "        \n",
    "        cpd_e=TabularCPD(variable='Enfermedad', variable_card=2, values=[self.beliefs['Enfermedad']])\n",
    "        cpd_tr=TabularCPD(variable='Tratamiento', variable_card=2, values=[self.beliefs['Tratamiento']])\n",
    "        cpd_r_tr=cpd_r_tr = TabularCPD(variable='Reaccion', variable_card=2, values=self.beliefs['Reaccion|Tratamiento'], evidence=['Tratamiento'],evidence_card=[2])\n",
    "        cpd_f_e_tr_r= TabularCPD(variable='Final', variable_card=2, values=self.beliefs['Final|Enfermedad, Tratamiento, Reaccion'],evidence=['Enfermedad', 'Tratamiento','Reaccion'],evidence_card=[2, 2,2])\n",
    "        model.add_cpds(cpd_e, cpd_tr, cpd_r_tr, cpd_f_e_tr_r)\n",
    "        \n",
    "        if (model.check_model()):\n",
    "            self.local_model=model\n",
    "            self.infer= VariableElimination(model)\n",
    "            print(\"Modelo Causal local creado\")\n",
    "            return(model,infer)\n",
    "        else:\n",
    "            print(\"Error en CPT\")\n",
    "        \n",
    "    def do_calculus(self,final,treatment): \n",
    "        #La distribuci贸n P(Final|do(Tratamiento))=P(Final|Enfermedad, Tratamiento, Reaccion)P(Reaccion|Tratamiento)P(Enfermedad)\n",
    "        #Treatment = 1 贸 0\n",
    "        #Quiero que me de la probabilidad de vivir=0 贸 =1 dado treatment=0 贸 =1\n",
    "        if (treatment==0):\n",
    "            res=self.beliefs['Final|Enfermedad, Tratamiento, Reaccion'][final][0]*self.beliefs['Reaccion|Tratamiento'][0][0]*self.beliefs['Enfermedad'][0] \\\n",
    "            +self.beliefs['Final|Enfermedad, Tratamiento, Reaccion'][final][1]*self.beliefs['Reaccion|Tratamiento'][1][0]*self.beliefs['Enfermedad'][0] \\\n",
    "            +self.beliefs['Final|Enfermedad, Tratamiento, Reaccion'][final][4]*self.beliefs['Reaccion|Tratamiento'][0][0]*self.beliefs['Enfermedad'][1] \\\n",
    "            +self.beliefs['Final|Enfermedad, Tratamiento, Reaccion'][final][5]*self.beliefs['Reaccion|Tratamiento'][1][0]*self.beliefs['Enfermedad'][1]\n",
    "        else:\n",
    "            res=self.beliefs['Final|Enfermedad, Tratamiento, Reaccion'][final][2]*self.beliefs['Reaccion|Tratamiento'][0][1]*self.beliefs['Enfermedad'][0] \\\n",
    "                +self.beliefs['Final|Enfermedad, Tratamiento, Reaccion'][final][3]*self.beliefs['Reaccion|Tratamiento'][1][1]*self.beliefs['Enfermedad'][0] \\\n",
    "                +self.beliefs['Final|Enfermedad, Tratamiento, Reaccion'][final][6]*self.beliefs['Reaccion|Tratamiento'][0][1]*self.beliefs['Enfermedad'][1] \\\n",
    "                +self.beliefs['Final|Enfermedad, Tratamiento, Reaccion'][final][7]*self.beliefs['Reaccion|Tratamiento'][1][1]*self.beliefs['Enfermedad'][1]\n",
    "        return(res)\n",
    "    \n",
    "    def make_decision(self,final=1):\n",
    "        f_0= self.do_calculus(final,0)\n",
    "        f_1=self.do_calculus(final,1)\n",
    "        if (f_0 > f_1):\n",
    "            return(0)\n",
    "        else:\n",
    "            return(1)\n",
    "        \n",
    "    def observing_external(self):\n",
    "        self.rounds=self.rounds+1\n",
    "        chosen_action=self.make_decision()\n",
    "        datos=self.simulator(chosen_action)\n",
    "        self.recompensa.append(datos['Final'])\n",
    "        return(datos)\n",
    "        \n",
    "    #def prob_data_given_beliefs(self,datos):\n",
    "        #Dados los beliefs, que son distribuciones, genero un modelo gr谩fico causal\n",
    "        #Este modelo gr谩fico causal da lugar a una distribuci贸n intervencional y puedo calcular\n",
    "        #la probabilidad de ver esa observaci贸n dadas las dirichlet\n",
    "        #result = [a * 8 for a in self.alpha]\n",
    "        \n",
    "        \n",
    "    def belief_updating(self):\n",
    "        datos=self.observing_external()\n",
    "        lista_datos=list(datos.values())\n",
    "        #self.alpha=list(map(add,self.alpha,self.conteo))\n",
    "        \n",
    "        #Conteo de enfermedad\n",
    "        if(lista_datos[0]==0):\n",
    "            self.conteo_enfermedad_A=self.conteo_enfermedad_A+1\n",
    "            self.alpha_enfermedad[0]=self.alpha_enfermedad[0]+self.conteo_enfermedad_A\n",
    "        else:\n",
    "            self.conteo_enfermedad_B=self.conteo_enfermedad_B+1\n",
    "            self.alpha_enfermedad[1]=self.alpha_enfermedad[0]+self.conteo_enfermedad_B\n",
    "        \n",
    "        #Conteo de tratamiento\n",
    "        if(lista_datos[1]==0):\n",
    "            self.conteo_tratamiento_f=self.conteo_tratamiento_f+1\n",
    "            self.alpha_tratamiento[0]=self.alpha_tratamiento[0]+self.conteo_tratamiento_f\n",
    "        else:\n",
    "            self.conteo_tratamiento_c=self.conteo_tratamiento_c+1\n",
    "            self.alpha_tratamiento[1]=self.alpha_tratamiento[0]+self.conteo_tratamiento_c\n",
    "        \n",
    "        #Conteo de reaccion dado tratamiento\n",
    "        if(lista_datos[1]==0 & lista_datos[2]==0):\n",
    "            self.conteo_reaccion_v_f=self.conteo_reaccion_v_f+1\n",
    "            self.alpha_reaccion_farmaco[0]=self.alpha_reaccion_farmaco[0]+self.conteo_reaccion_v_f\n",
    "        if(lista_datos[1]==0 & lista_datos[2]==1):\n",
    "            self.conteo_reaccion_m_f=self.conteo_reaccion_m_f+1\n",
    "            self.alpha_reaccion_farmaco[1]=self.alpha_reaccion_farmaco[1]+self.conteo_reaccion_m_f\n",
    "        \n",
    "        if(lista_datos[1]==1 & lista_datos[2]==0):\n",
    "            self.conteo_reaccion_v_c=self.conteo_reaccion_v_c+1\n",
    "            self.alpha_reaccion_cirugia[0]+self.conteo_reaccion_v_c\n",
    "        if(lista_datos[1]==0 & lista_datos[2]==1):\n",
    "            self.conteo_reaccion_m_c=self.conteo_reaccion_m_c+1\n",
    "            self.alpha_reaccion_cirugia[1]=self.alpha_reaccion_cirugia[1]+self.conteo_reaccion_m_c\n",
    "        \n",
    "        #Conteo de vivir dado enfermedad, tratamiento y reaccion\n",
    "        #Para vivir\n",
    "        if(lista_datos[0]==0 & lista_datos[1]==0 & lista_datos[2]==0 & lista_datos[3]==0):\n",
    "            self.conteo_v_A_f_v=self.conteo_v_A_f_v+1\n",
    "            self.alpha_A_f_v[0]=self.alpha_A_f_v[0]+self.conteo_v_A_f_v\n",
    "            \n",
    "        if(lista_datos[0]==0 & lista_datos[1]==0 & lista_datos[2]==1 & lista_datos[3]==0):\n",
    "            self.conteo_v_A_f_m=self.conteo_v_A_f_m+1\n",
    "            self.alpha_A_f_m[0]=self.alpha_A_f_m[0]+self.conteo_v_A_f_m\n",
    "            \n",
    "        if(lista_datos[0]==0 & lista_datos[1]==1 & lista_datos[2]==0 & lista_datos[3]==0):\n",
    "            self.conteo_v_A_c_v=self.conteo_v_A_c_v+1\n",
    "            self.alpha_A_c_v[0]=self.alpha_A_c_v[0]+self.conteo_v_A_c_v\n",
    "            \n",
    "        if(lista_datos[0]==0 & lista_datos[1]==1 & lista_datos[2]==1 & lista_datos[3]==0):\n",
    "            self.conteo_v_A_c_m=self.conteo_v_A_c_m+1\n",
    "            self.alpha_A_c_m[0]=self.alpha_A_c_m[0]+self.conteo_v_A_c_m\n",
    "        \n",
    "        if(lista_datos[0]==1 & lista_datos[1]==0 & lista_datos[2]==0 & lista_datos[3]==0):\n",
    "            self.conteo_v_B_f_v=self.conteo_v_B_f_v+1\n",
    "            self.alpha_B_f_v[0]=self.alpha_B_f_v[0]+self.conteo_v_B_f_v\n",
    "        \n",
    "        if(lista_datos[0]==1 & lista_datos[1]==0 & lista_datos[2]==1 & lista_datos[3]==0):\n",
    "            self.conteo_v_B_f_m=self.conteo_v_B_f_m+1\n",
    "            self.alpha_B_f_m[0]=self.alpha_B_f_m[0]+self.conteo_v_B_f_m\n",
    "            \n",
    "        if(lista_datos[0]==1 & lista_datos[1]==1 & lista_datos[2]==0 & lista_datos[3]==0):\n",
    "            self.conteo_v_B_c_v=self.conteo_v_B_c_v+1\n",
    "            self.alpha_B_c_v[0]=self.alpha_B_c_v[0]+self.conteo_v_B_c_v\n",
    "            \n",
    "        if(lista_datos[0]==1 & lista_datos[1]==1 & lista_datos[2]==1 & lista_datos[3]==0):\n",
    "            self.conteo_v_B_c_m=self.conteo_v_B_c_m+1\n",
    "            self.alpha_B_c_m[0]=self.alpha_B_c_m[0]+self.conteo_v_B_c_m\n",
    "        #Para morir\n",
    "        if(lista_datos[0]==0 & lista_datos[1]==0 & lista_datos[2]==0 & lista_datos[3]==1):\n",
    "            self.conteo_m_A_f_v=self.conteo_m_A_f_v+1\n",
    "            self.alpha_A_f_v[1]=self.alpha_A_f_v[1]+self.conteo_m_A_f_v\n",
    "            \n",
    "        if(lista_datos[0]==0 & lista_datos[1]==0 & lista_datos[2]==1 & lista_datos[3]==1):     \n",
    "            self.conteo_m_A_f_m=self.conteo_m_A_f_m+1\n",
    "            self.alpha_A_f_m[1]=self.alpha_A_f_m[1]+self.conteo_m_A_f_m\n",
    "            \n",
    "        if(lista_datos[0]==0 & lista_datos[1]==1 & lista_datos[2]==0 & lista_datos[3]==1):    \n",
    "            self.conteo_m_A_c_v=self.conteo_m_A_c_v+1\n",
    "            self.alpha_A_c_v[1]=self.alpha_A_c_v[1]+self.conteo_m_A_c_v\n",
    "            \n",
    "        if(lista_datos[0]==0 & lista_datos[1]==1 & lista_datos[2]==1 & lista_datos[3]==1):\n",
    "            self.conteo_m_A_c_m=self.conteo_m_A_c_m+1\n",
    "            self.alpha_A_c_m[1]=self.alpha_A_c_m[1]+self.conteo_m_A_c_m\n",
    "            \n",
    "        if(lista_datos[0]==1 & lista_datos[1]==0 & lista_datos[2]==0 & lista_datos[3]==1):\n",
    "            self.conteo_m_B_f_v=self.conteo_m_B_f_v+1\n",
    "            self.alpha_B_f_v[1]=self.alpha_B_f_v[1]+self.conteo_m_B_f_v\n",
    "            \n",
    "        if(lista_datos[0]==1 & lista_datos[1]==0 & lista_datos[2]==1 & lista_datos[3]==1):\n",
    "            self.conteo_m_B_f_m=self.conteo_m_B_f_m+1\n",
    "            self.alpha_B_f_m[1]=self.alpha_B_f_m[1]+self.conteo_m_B_f_m\n",
    "            \n",
    "        if(lista_datos[0]==1 & lista_datos[1]==1 & lista_datos[2]==0 & lista_datos[3]==1):\n",
    "            self.conteo_m_B_c_v=self.conteo_m_B_c_v+1\n",
    "            self.alpha_B_c_v[1]=self.alpha_B_c_v[1]+self.conteo_m_B_c_v\n",
    "            \n",
    "        if(lista_datos[0]==1 & lista_datos[1]==1 & lista_datos[2]==1 & lista_datos[3]==0):\n",
    "            self.conteo_m_B_c_m=self.conteo_m_B_c_m+1\n",
    "            self.alpha_B_c_m[1]=self.alpha_B_c_m[1]+self.conteo_m_B_c_m\n",
    "        \n",
    "        \n",
    "        self.beliefs=self.belief_formation()\n",
    "        \n",
    "    def get_reward(self):\n",
    "        return(self.recompensa)\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class random_agent:\n",
    "    def __init__(self,true_model,model_simulator, max_variables, max_valores):\n",
    "        self.recompensa=[0]\n",
    "        self.simulator=model_simulator.action_simulator\n",
    "    \n",
    "    def make_decision(self):\n",
    "        p=list(np.random.random_sample(1))\n",
    "        if (p[0] > 0.5):\n",
    "            return(0)\n",
    "        else:\n",
    "            return(1)\n",
    "        \n",
    "    def observing_external(self):\n",
    "        chosen_action=self.make_decision()\n",
    "        datos=self.simulator(chosen_action)\n",
    "        self.recompensa.append(datos['Final'])\n",
    "        return(datos)\n",
    "    \n",
    "    def get_reward(self):\n",
    "        return(self.recompensa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_learning_agent:\n",
    "    def __init__(self, model_simulator, max_valores,epsilon):\n",
    "        self.epsilon = epsilon\n",
    "        self.simulator=model_simulator.action_simulator\n",
    "        self.K=max_valores\n",
    "        self.k = np.zeros(self.K, dtype=np.int)  # number of times action was chosen\n",
    "        self.Q = np.zeros(self.K, dtype=np.float)  # estimated value\n",
    "        self.reward_history = []\n",
    "\n",
    "    # Update Q action-value using:\n",
    "    # Q(a) <- Q(a) + 1/(k+1) * (r(a) - Q(a))\n",
    "    def update_Q(self, action, reward):\n",
    "        self.k[action] += 1  # update action counter k -> k+1\n",
    "        self.Q[action] += (1./self.k[action]) * (reward - self.Q[action])\n",
    "\n",
    "        # Choose action using an epsilon-greedy agent\n",
    "    def choose_action(self, force_explore=False):\n",
    "        rand = np.random.random()  # [0.0,1.0)\n",
    "        if (rand < self.epsilon) or force_explore:\n",
    "            action_explore = np.random.randint(self.K)  # explore random bandit\n",
    "            return action_explore\n",
    "        else:\n",
    "            #action_greedy = np.argmax(self.Q)  # exploit best current bandit\n",
    "            action_greedy = np.random.choice(np.flatnonzero(self.Q == self.Q.max()))\n",
    "            return action_greedy\n",
    "    \n",
    "    def observe_external(self):\n",
    "        chosen_action=self.choose_action()\n",
    "        datos=self.simulator(chosen_action)\n",
    "        reward=datos['Final']\n",
    "        self.update_Q(chosen_action,reward)\n",
    "        self.reward_history.append(reward)\n",
    "        \n",
    "    def get_reward(self):\n",
    "        return(self.reward_history)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Uso"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo=true_causal_model(4,infer,sampling,model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agente_random=random_agent(model,modelo,4,2)\n",
    "#                (self, true_model,model_simulator, max_variables, max_valores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agente=causal_agent(model,modelo,4,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "agente.belief_formation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agente.print_beliefs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agente.fix_causal_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agente.belief_updating()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agente.fix_causal_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agente_causal.local_model.get_cpds('Enfermedad'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agente.get_reward()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experimentos num茅ricos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Causal agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=100\n",
    "modelo=true_causal_model(4,infer,sampling,model)\n",
    "agente_causal=causal_agent(model,modelo,4,2)\n",
    "agente_causal.belief_formation()\n",
    "recompensa_causal=np.zeros(N,dtype='float')\n",
    "recompensa_promedio_causal=np.zeros(N,dtype='float')\n",
    "for i in range(N):\n",
    "    agente_causal.belief_updating()\n",
    "    agente_causal.fix_causal_model()\n",
    "recompensas_causal=agente_causal.get_reward()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agente_causal.local_model.get_cpds('Enfermedad'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(recompensas_causal)\n",
    "plt.xlim([1, N])\n",
    "plt.ylim([0, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recompensa_acumulada_causal=np.cumsum(recompensas_causal)\n",
    "recompensa_promedio_causal=recompensa_acumulada_causal/N\n",
    "plt.plot(recompensa_promedio_causal)\n",
    "plt.xlabel(\"Number Rounds\")\n",
    "plt.ylabel(\"Average Reward for Causal Agent\".format())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recompensa_acumulada_causal"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "modelo=true_causal_model(4,infer,sampling,model)\n",
    "agente_random=random_agent(model,modelo,4,2)\n",
    "recompensa_random=np.zeros(N,dtype='float')\n",
    "recompensa_promedio_random=np.zeros(N,dtype='float')\n",
    "for i in range(N):\n",
    "    agente_random.observing_external()\n",
    "recompensas_random=agente_random.get_reward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(recompensas_random)\n",
    "plt.xlim([1, N])\n",
    "plt.ylim([0, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recompensa_acumulada_random=np.cumsum(recompensas_random)\n",
    "recompensa_promedio_random=recompensa_acumulada_random/N\n",
    "plt.plot(recompensa_promedio_random)\n",
    "plt.xlabel(\"Rounds for random agent\")\n",
    "plt.ylabel(\"Average Reward for random agent\".format())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo=true_causal_model(4,infer,sampling,model)\n",
    "agente_Q=Q_learning_agent(modelo,2,0.3)\n",
    "recompensa_Q=np.zeros(N,dtype='float')\n",
    "recompensa_Q=np.zeros(N,dtype='float')\n",
    "for i in range(N):\n",
    "    agente_Q.observe_external()\n",
    "    \n",
    "recompensas_Q=agente_Q.get_reward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(recompensas_Q)\n",
    "plt.xlim([1, N])\n",
    "plt.ylim([0, 5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recompensa_acumulada_Q=np.cumsum(recompensas_Q)\n",
    "recompensa_promedio_Q=recompensa_acumulada_Q/N\n",
    "plt.plot(recompensa_promedio_Q)\n",
    "plt.xlabel(\"Rounds for Q-learning agent\")\n",
    "plt.ylabel(\"Average Reward for Q-learning agent\".format())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Todos juntos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(recompensa_promedio_causal, label='Causal agent')\n",
    "plt.plot(recompensa_promedio_random,linestyle=':',label='Random agent')\n",
    "plt.plot(recompensa_promedio_Q,linestyle='--',label='Q agent')\n",
    "leg = plt.legend(loc='upper left', shadow=True, fontsize=10)\n",
    "plt.xlim([1, N])\n",
    "plt.ylim([0, 1])\n",
    "plt.ylabel(\"Average Reward for agent\")\n",
    "plt.xlabel(\"Rounds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recompensa_promedio_causal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recompensa_acumulada_causal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agente_informado=fully_informed_agent(modelo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agente_informado.make_decision()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agente_informado.observing_external()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agente_informado.get_reward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N=20\n",
    "modelo=true_causal_model(4,infer,sampling,model)\n",
    "agente_informado=fully_informed_agent(modelo)\n",
    "recompensa_informada=np.zeros(N,dtype='float')\n",
    "recompensa_promedio_informada=np.zeros(N,dtype='float')\n",
    "for i in range(N):\n",
    "    agente_informado.observing_external()\n",
    "recompensas_informado=agente_informado.get_reward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recompensas_informado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(recompensas_informado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "recompensa_acumulada_informado=np.cumsum(recompensas_informado)\n",
    "recompensa_promedio_informado=recompensa_acumulada_informado/N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(recompensa_promedio_informado)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluando el modelo causal aprendido\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agente_causal.local_model.get_cpds('Enfermedad'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agente_causal.local_model.get_cpds('Tratamiento'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agente_causal.local_model.get_cpds('Final'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(model.get_cpds('Final'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prueba de la creacion del modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo=true_causal_model(4,infer,sampling,model)\n",
    "agente_causal=causal_agent(model,modelo,4,2)\n",
    "agente_causal.belief_formation()\n",
    "print(agente_causal.local_model.get_cpds('Enfermedad'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agente_causal.belief_updating()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agente_causal.fix_causal_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(agente_causal.local_model.get_cpds('Enfermedad'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "elements = [0,1]\n",
    "probabilities = [0.4,0.6]\n",
    "res=np.random.choice(elements, 1, p=probabilities)\n",
    "res[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.randint(0,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agente_causal.print_beliefs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelo.action_simulator(1)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
